run:
  name: 'prepare-task-data'
  results_dir: ${data_curation.run.results_dir}/${.name}
  dependency: "singleton"
  time_limit: "04:00:00"
  nodes: 2
  node_type: cpu

output_task_ngrams: ${.run.results_dir}/task_ngrams.pkl
# The below flag skips computation of task n-grams if the file above is already present
# Set to False if you want to recompute task n-grams with different tasks
use_ngram_cache: True

lm_tasks_config:
  tasks:
    # The Python modules below define language model downstream evaluation
    # task data. If one of the below tasks is specified, N-grams will 
    # be constructed from the documents that make up the task data
    # using the script prepare_task_data.
    # find_matching_ngrams will then search for these N-grams
    # in the training documents, and remove_matching_ngrams will
    # split the documents based on matches
    - name: ndc.tasks.Winogrande
      params: {}
    - name: ndc.tasks.Squad
      params: {}
    - name: ndc.tasks.TriviaQA
      params: {}
    - name: ndc.tasks.Quac
      params: {}
    - name: ndc.tasks.WebQA
      params: {}
    - name: ndc.tasks.Race
      params: {}
    - name: ndc.tasks.Drop
      params: {}
    - name: ndc.tasks.WiC
      params: {}
    - name: ndc.tasks.PIQA
      params: {}
    - name: ndc.tasks.ArcEasy
      params: {}
    - name: ndc.tasks.ArcChallenge
      params: {}
    - name: ndc.tasks.OpenBookQA
      params: {}
    - name: ndc.tasks.BoolQ
      params: {}
    - name: ndc.tasks.Copa
      params: {}
    - name: ndc.tasks.RTE
      params: {}
    - name: ndc.tasks.MultiRC
      params: {}
    - name: ndc.tasks.WSC
      params: {}
    - name: ndc.tasks.CB
      params: {}
    - name: ndc.tasks.ANLI
      params: {}
    - name: ndc.tasks.Record
      params: {}
