USR_DIR="/gpfs/fs1/projects/ent_joc/users/jbieniusiewi"
LAUNCHER_DIR="/gpfs/fs1/projects/ent_joc/users/jbieniusiewi/ft/NeMo-Megatron-Launcher"

# create dummy data this that is required by the launcher
# we will use mock data
mkdir -p ${LAUNCHER_DIR}/dummy_data_dir


# USE SC2 container, but train GPT3 5b

NVTE_APPLY_QK_LAYER_SCALING=1 HYDRA_FULL_ERROR=1 PYTHONWARNINGS="ignore" HUGGINGFACE_HUB_CACHE=/root/.cache/huggingface/hub  python3 ${LAUNCHER_DIR}/launcher_scripts/main.py \
    training=gpt3/5b \
    stages=["training"] \
    numa_mapping.enable=True \
    data_dir=${LAUNCHER_DIR}/dummy_data_dir \
    training.model.data.data_impl="mock" \
    training.model.data.data_prefix=[] \
    launcher_scripts_path=${LAUNCHER_DIR}/launcher_scripts  \
    container_mounts=[$USR_DIR:$USR_DIR] \
    container="gitlab-master.nvidia.com/dl/nemo/nemo-fw/train:sc2_fault_tol_elastic" \
    cluster.partition=batch_short_dgx1_m2 \
    cluster.account=coreai_dlalgo_llm \
    cluster.job_name_prefix="coreai_dlalgo_llm-test-ft5b:" \
    cluster.gpus_per_task=null \
    cluster.gpus_per_node=null \
    ++cluster.nv_meta="ml-model.fault_tol_tests" \
    ++cluster.gres="gpu:8" \
    ++cluster.signal="TERM@180" \
    training.exp_manager.resume_if_exists=True \
    training.exp_manager.create_checkpoint_callback=True \
    training.exp_manager.checkpoint_callback_params.save_top_k=1 \
    training.exp_manager.resume_ignore_no_checkpoint=True \
    training.run.name="fault_tol_gpt3_5b_dbg" \
    training.run.time_limit=00:30:00 \
    training.trainer.max_time=00:01:00:00 \
    training.trainer.num_nodes=4 \
    training.trainer.devices=8 \
    training.trainer.log_every_n_steps=10 \
    training.trainer.val_check_interval=400 \
    ++training.trainer.precision=16 \
    ++training.model.mcore_gpt=False \
    ++training.model.tokenizer.merge_file="/gpfs/fs1/projects/gpu_adlr/datasets/nlp/gpt3/bpe/gpt2-merges.txt" \
    ++training.model.tokenizer.vocab_file="/gpfs/fs1/projects/gpu_adlr/datasets/nlp/gpt3/bpe/gpt2-vocab.txt" \
    training.trainer.enable_checkpointing=False \
    training.model.micro_batch_size=1 \
    training.model.global_batch_size=4 \
    training.model.tensor_model_parallel_size=8 \
    training.model.pipeline_model_parallel_size=1 \
    ++training.exp_manager.create_fault_tolerance_callback=True \
    ++training.exp_manager.fault_tolerance.initial_rank_heartbeat_timeout=null \
    ++training.exp_manager.fault_tolerance.rank_heartbeat_timeout=null \
    ++training.exp_manager.fault_tolerance.max_subsequent_job_failures=3 \
    ++training.exp_manager.fault_tolerance.max_rank_restarts=0


#
#    ++training.exp_manager.fault_tolerance.simulated_fault.fault_type=random \
#    ++training.exp_manager.fault_tolerance.simulated_fault.base_delay=900
#
