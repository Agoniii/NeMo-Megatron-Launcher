USR_DIR="/lustre/fsw/coreai_dlalgo_llm/jbieniusiewi"
LAUNCHER_DIR="/lustre/fsw/coreai_dlalgo_llm/jbieniusiewi/nemo/NeMo-Megatron-Launcher/"

HYDRA_FULL_ERROR=1 PYTHONWARNINGS="ignore" python3 ${LAUNCHER_DIR}/launcher_scripts/main.py \
    training=gpt3/starcoder2_3b \
    stages=["training"] \
    numa_mapping.enable=True \
    data_dir=/lustre/fsw/coreai_dlalgo_llm/aot/datasets/stack_v2_final_tokenized \
    launcher_scripts_path=${LAUNCHER_DIR}/launcher_scripts  \
    container_mounts=[$USR_DIR:$USR_DIR] \
    container="gitlab-master.nvidia.com/dl/nemo/nemo-fw/train:sc2_fault_tol" \
    cluster.partition=batch \
    cluster.account=coreai_dlalgo_llm \
    cluster.job_name_prefix="coreai_dlalgo_llm-sc2_3b-ft:" \
    cluster.gpus_per_task=null \
    cluster.gpus_per_node=null \
    training.run.name="fault_tol_sc2_3b" \
    training.run.time_limit=04:00:00 \
    training.trainer.max_time=00:04:00:00 \
    training.trainer.num_nodes=2 \
    training.trainer.devices=8 \
    training.trainer.log_every_n_steps=1 \
    training.trainer.val_check_interval=1000 \
    ++training.exp_manager.fault_tolerance.initial_rank_heartbeat_timeout=720 \
    ++training.exp_manager.fault_tolerance.rank_heartbeat_timeout=600 \
    ++training.exp_manager.fault_tolerance.ipc_timeout=60 \
    ++training.exp_manager.fault_tolerance.rank_termination_signal=9 \
    ++training.exp_manager.fault_tolerance.autoresume_if_faulted=True \
    ++training.exp_manager.autoresume_if_preempted=False


# Uncomment to test simulated faults
#    ++training.exp_manager.fault_tolerance.simulated_fault.fault_type=random \
#    ++training.exp_manager.fault_tolerance.simulated_fault.base_delay=1800
